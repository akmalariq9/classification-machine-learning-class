{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop('Crop', axis=1)\n",
    "y_train = train_data['Crop']\n",
    "X_test = test_data.drop('Crop', axis=1)\n",
    "y_test = test_data['Crop']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.90262091\n",
      "Iteration 2, loss = 2.68505728\n",
      "Iteration 3, loss = 2.46631640\n",
      "Iteration 4, loss = 2.23120700\n",
      "Iteration 5, loss = 1.97704689\n",
      "Iteration 6, loss = 1.71173312\n",
      "Iteration 7, loss = 1.44168294\n",
      "Iteration 8, loss = 1.18569286\n",
      "Iteration 9, loss = 0.96059544\n",
      "Iteration 10, loss = 0.77854172\n",
      "Iteration 11, loss = 0.63695193\n",
      "Iteration 12, loss = 0.53176417\n",
      "Iteration 13, loss = 0.45131635\n",
      "Iteration 14, loss = 0.39188152\n",
      "Iteration 15, loss = 0.34489335\n",
      "Iteration 16, loss = 0.30812200\n",
      "Iteration 17, loss = 0.27920041\n",
      "Iteration 18, loss = 0.25504744\n",
      "Iteration 19, loss = 0.23466613\n",
      "Iteration 20, loss = 0.21792132\n",
      "Iteration 21, loss = 0.20301896\n",
      "Iteration 22, loss = 0.19154005\n",
      "Iteration 23, loss = 0.17923949\n",
      "Iteration 24, loss = 0.16955179\n",
      "Iteration 25, loss = 0.16080709\n",
      "Iteration 26, loss = 0.15261184\n",
      "Iteration 27, loss = 0.14495766\n",
      "Iteration 28, loss = 0.13815297\n",
      "Iteration 29, loss = 0.13279424\n",
      "Iteration 30, loss = 0.12727215\n",
      "Iteration 31, loss = 0.12264854\n",
      "Iteration 32, loss = 0.11751159\n",
      "Iteration 33, loss = 0.11304844\n",
      "Iteration 34, loss = 0.10920261\n",
      "Iteration 35, loss = 0.10552748\n",
      "Iteration 36, loss = 0.10143406\n",
      "Iteration 37, loss = 0.09865070\n",
      "Iteration 38, loss = 0.09484207\n",
      "Iteration 39, loss = 0.09224016\n",
      "Iteration 40, loss = 0.08984560\n",
      "Iteration 41, loss = 0.08655424\n",
      "Iteration 42, loss = 0.08422811\n",
      "Iteration 43, loss = 0.08236237\n",
      "Iteration 44, loss = 0.08080360\n",
      "Iteration 45, loss = 0.07784330\n",
      "Iteration 46, loss = 0.07618051\n",
      "Iteration 47, loss = 0.07355120\n",
      "Iteration 48, loss = 0.07230842\n",
      "Iteration 49, loss = 0.06977048\n",
      "Iteration 50, loss = 0.06902316\n",
      "Iteration 51, loss = 0.06768873\n",
      "Iteration 52, loss = 0.06718890\n",
      "Iteration 53, loss = 0.06408562\n",
      "Iteration 54, loss = 0.06281077\n",
      "Iteration 55, loss = 0.06138963\n",
      "Iteration 56, loss = 0.06007133\n",
      "Iteration 57, loss = 0.05796766\n",
      "Iteration 58, loss = 0.05737376\n",
      "Iteration 59, loss = 0.05697656\n",
      "Iteration 60, loss = 0.05488208\n",
      "Iteration 61, loss = 0.05402368\n",
      "Iteration 62, loss = 0.05255109\n",
      "Iteration 63, loss = 0.05185331\n",
      "Iteration 64, loss = 0.05020353\n",
      "Iteration 65, loss = 0.04975707\n",
      "Iteration 66, loss = 0.04943694\n",
      "Iteration 67, loss = 0.04844914\n",
      "Iteration 68, loss = 0.04777501\n",
      "Iteration 69, loss = 0.04555900\n",
      "Iteration 70, loss = 0.04470196\n",
      "Iteration 71, loss = 0.04385779\n",
      "Iteration 72, loss = 0.04289012\n",
      "Iteration 73, loss = 0.04212924\n",
      "Iteration 74, loss = 0.04120676\n",
      "Iteration 75, loss = 0.04088414\n",
      "Iteration 76, loss = 0.04023957\n",
      "Iteration 77, loss = 0.03913284\n",
      "Iteration 78, loss = 0.03890587\n",
      "Iteration 79, loss = 0.03789866\n",
      "Iteration 80, loss = 0.03714393\n",
      "Iteration 81, loss = 0.03658264\n",
      "Iteration 82, loss = 0.03628535\n",
      "Iteration 83, loss = 0.03647047\n",
      "Iteration 84, loss = 0.03420645\n",
      "Iteration 85, loss = 0.03409778\n",
      "Iteration 86, loss = 0.03343553\n",
      "Iteration 87, loss = 0.03401006\n",
      "Iteration 88, loss = 0.03233080\n",
      "Iteration 89, loss = 0.03214676\n",
      "Iteration 90, loss = 0.03111790\n",
      "Iteration 91, loss = 0.03084349\n",
      "Iteration 92, loss = 0.03064958\n",
      "Iteration 93, loss = 0.03008557\n",
      "Iteration 94, loss = 0.02971556\n",
      "Iteration 95, loss = 0.02987955\n",
      "Iteration 96, loss = 0.02965103\n",
      "Iteration 97, loss = 0.02800642\n",
      "Iteration 98, loss = 0.02755368\n",
      "Iteration 99, loss = 0.02691983\n",
      "Iteration 100, loss = 0.02680571\n",
      "Iteration 101, loss = 0.02611989\n",
      "Iteration 102, loss = 0.02615782\n",
      "Iteration 103, loss = 0.02595393\n",
      "Iteration 104, loss = 0.02514939\n",
      "Iteration 105, loss = 0.02445051\n",
      "Iteration 106, loss = 0.02447584\n",
      "Iteration 107, loss = 0.02478667\n",
      "Iteration 108, loss = 0.02419440\n",
      "Iteration 109, loss = 0.02353980\n",
      "Iteration 110, loss = 0.02289093\n",
      "Iteration 111, loss = 0.02236887\n",
      "Iteration 112, loss = 0.02235510\n",
      "Iteration 113, loss = 0.02227605\n",
      "Iteration 114, loss = 0.02207632\n",
      "Iteration 115, loss = 0.02219261\n",
      "Iteration 116, loss = 0.02269315\n",
      "Iteration 117, loss = 0.02171635\n",
      "Iteration 118, loss = 0.02051970\n",
      "Iteration 119, loss = 0.02039322\n",
      "Iteration 120, loss = 0.02026165\n",
      "Iteration 121, loss = 0.02050566\n",
      "Iteration 122, loss = 0.01944768\n",
      "Iteration 123, loss = 0.02006111\n",
      "Iteration 124, loss = 0.01980157\n",
      "Iteration 125, loss = 0.01895549\n",
      "Iteration 126, loss = 0.01896403\n",
      "Iteration 127, loss = 0.01833010\n",
      "Iteration 128, loss = 0.01860576\n",
      "Iteration 129, loss = 0.01924074\n",
      "Iteration 130, loss = 0.01762161\n",
      "Iteration 131, loss = 0.01778752\n",
      "Iteration 132, loss = 0.01757255\n",
      "Iteration 133, loss = 0.01725212\n",
      "Iteration 134, loss = 0.01727985\n",
      "Iteration 135, loss = 0.01732824\n",
      "Iteration 136, loss = 0.01695489\n",
      "Iteration 137, loss = 0.01644446\n",
      "Iteration 138, loss = 0.01600622\n",
      "Iteration 139, loss = 0.01639399\n",
      "Iteration 140, loss = 0.01566503\n",
      "Iteration 141, loss = 0.01551289\n",
      "Iteration 142, loss = 0.01579397\n",
      "Iteration 143, loss = 0.01565335\n",
      "Iteration 144, loss = 0.01502679\n",
      "Iteration 145, loss = 0.01526453\n",
      "Iteration 146, loss = 0.01504067\n",
      "Iteration 147, loss = 0.01504191\n",
      "Iteration 148, loss = 0.01480590\n",
      "Iteration 149, loss = 0.01445947\n",
      "Iteration 150, loss = 0.01465785\n",
      "Iteration 151, loss = 0.01424989\n",
      "Iteration 152, loss = 0.01388488\n",
      "Iteration 153, loss = 0.01398038\n",
      "Iteration 154, loss = 0.01342227\n",
      "Iteration 155, loss = 0.01345911\n",
      "Iteration 156, loss = 0.01321169\n",
      "Iteration 157, loss = 0.01296499\n",
      "Iteration 158, loss = 0.01327878\n",
      "Iteration 159, loss = 0.01289448\n",
      "Iteration 160, loss = 0.01265256\n",
      "Iteration 161, loss = 0.01293313\n",
      "Iteration 162, loss = 0.01271333\n",
      "Iteration 163, loss = 0.01239387\n",
      "Iteration 164, loss = 0.01277859\n",
      "Iteration 165, loss = 0.01274934\n",
      "Iteration 166, loss = 0.01189142\n",
      "Iteration 167, loss = 0.01184257\n",
      "Iteration 168, loss = 0.01161594\n",
      "Iteration 169, loss = 0.01170646\n",
      "Iteration 170, loss = 0.01143927\n",
      "Iteration 171, loss = 0.01125798\n",
      "Iteration 172, loss = 0.01128369\n",
      "Iteration 173, loss = 0.01117137\n",
      "Iteration 174, loss = 0.01133300\n",
      "Iteration 175, loss = 0.01098505\n",
      "Iteration 176, loss = 0.01086684\n",
      "Iteration 177, loss = 0.01079266\n",
      "Iteration 178, loss = 0.01053455\n",
      "Iteration 179, loss = 0.01045869\n",
      "Iteration 180, loss = 0.01044021\n",
      "Iteration 181, loss = 0.01033197\n",
      "Iteration 182, loss = 0.01043433\n",
      "Iteration 183, loss = 0.01106442\n",
      "Iteration 184, loss = 0.01001974\n",
      "Iteration 185, loss = 0.01027966\n",
      "Iteration 186, loss = 0.01018704\n",
      "Iteration 187, loss = 0.01007068\n",
      "Iteration 188, loss = 0.00955707\n",
      "Iteration 189, loss = 0.01000554\n",
      "Iteration 190, loss = 0.00943807\n",
      "Iteration 191, loss = 0.00940121\n",
      "Iteration 192, loss = 0.00916496\n",
      "Iteration 193, loss = 0.00925903\n",
      "Iteration 194, loss = 0.00931368\n",
      "Iteration 195, loss = 0.00969871\n",
      "Iteration 196, loss = 0.00934552\n",
      "Iteration 197, loss = 0.00922401\n",
      "Iteration 198, loss = 0.00975956\n",
      "Iteration 199, loss = 0.00890334\n",
      "Iteration 200, loss = 0.00954160\n",
      "Iteration 201, loss = 0.00967710\n",
      "Iteration 202, loss = 0.00820709\n",
      "Iteration 203, loss = 0.00921028\n",
      "Iteration 204, loss = 0.00902786\n",
      "Iteration 205, loss = 0.00881382\n",
      "Iteration 206, loss = 0.00888903\n",
      "Iteration 207, loss = 0.00816970\n",
      "Iteration 208, loss = 0.00843794\n",
      "Iteration 209, loss = 0.00800611\n",
      "Iteration 210, loss = 0.00852444\n",
      "Iteration 211, loss = 0.00785233\n",
      "Iteration 212, loss = 0.00797165\n",
      "Iteration 213, loss = 0.00768203\n",
      "Iteration 214, loss = 0.00788474\n",
      "Iteration 215, loss = 0.00752259\n",
      "Iteration 216, loss = 0.00747176\n",
      "Iteration 217, loss = 0.00773137\n",
      "Iteration 218, loss = 0.00730326\n",
      "Iteration 219, loss = 0.00715856\n",
      "Iteration 220, loss = 0.00758086\n",
      "Iteration 221, loss = 0.00763446\n",
      "Iteration 222, loss = 0.00726916\n",
      "Iteration 223, loss = 0.00699753\n",
      "Iteration 224, loss = 0.00691378\n",
      "Iteration 225, loss = 0.00675639\n",
      "Iteration 226, loss = 0.00683095\n",
      "Iteration 227, loss = 0.00664009\n",
      "Iteration 228, loss = 0.00684707\n",
      "Iteration 229, loss = 0.00666269\n",
      "Iteration 230, loss = 0.00667924\n",
      "Iteration 231, loss = 0.00645866\n",
      "Iteration 232, loss = 0.00684695\n",
      "Iteration 233, loss = 0.00624131\n",
      "Iteration 234, loss = 0.00661130\n",
      "Iteration 235, loss = 0.00672308\n",
      "Iteration 236, loss = 0.00655950\n",
      "Iteration 237, loss = 0.00613346\n",
      "Iteration 238, loss = 0.00638119\n",
      "Iteration 239, loss = 0.00619669\n",
      "Iteration 240, loss = 0.00619573\n",
      "Iteration 241, loss = 0.00629245\n",
      "Iteration 242, loss = 0.00606901\n",
      "Iteration 243, loss = 0.00609557\n",
      "Iteration 244, loss = 0.00589400\n",
      "Iteration 245, loss = 0.00577547\n",
      "Iteration 246, loss = 0.00576686\n",
      "Iteration 247, loss = 0.00607193\n",
      "Iteration 248, loss = 0.00592513\n",
      "Iteration 249, loss = 0.00571947\n",
      "Iteration 250, loss = 0.00581437\n",
      "Iteration 251, loss = 0.00572393\n",
      "Iteration 252, loss = 0.00541056\n",
      "Iteration 253, loss = 0.00547398\n",
      "Iteration 254, loss = 0.00550530\n",
      "Iteration 255, loss = 0.00557028\n",
      "Iteration 256, loss = 0.00561717\n",
      "Iteration 257, loss = 0.00554167\n",
      "Iteration 258, loss = 0.00556177\n",
      "Iteration 259, loss = 0.00502435\n",
      "Iteration 260, loss = 0.00543444\n",
      "Iteration 261, loss = 0.00555901\n",
      "Iteration 262, loss = 0.00531509\n",
      "Iteration 263, loss = 0.00505514\n",
      "Iteration 264, loss = 0.00533468\n",
      "Iteration 265, loss = 0.00584176\n",
      "Iteration 266, loss = 0.00557093\n",
      "Iteration 267, loss = 0.00473120\n",
      "Iteration 268, loss = 0.00486428\n",
      "Iteration 269, loss = 0.00502672\n",
      "Iteration 270, loss = 0.00484650\n",
      "Iteration 271, loss = 0.00461617\n",
      "Iteration 272, loss = 0.00475604\n",
      "Iteration 273, loss = 0.00470570\n",
      "Iteration 274, loss = 0.00459996\n",
      "Iteration 275, loss = 0.00468859\n",
      "Iteration 276, loss = 0.00478693\n",
      "Iteration 277, loss = 0.00451461\n",
      "Iteration 278, loss = 0.00473437\n",
      "Iteration 279, loss = 0.00435618\n",
      "Iteration 280, loss = 0.00464132\n",
      "Iteration 281, loss = 0.00419289\n",
      "Iteration 282, loss = 0.00475365\n",
      "Iteration 283, loss = 0.00508303\n",
      "Iteration 284, loss = 0.00432412\n",
      "Iteration 285, loss = 0.00415263\n",
      "Iteration 286, loss = 0.00452941\n",
      "Iteration 287, loss = 0.00435411\n",
      "Iteration 288, loss = 0.00393825\n",
      "Iteration 289, loss = 0.00414435\n",
      "Iteration 290, loss = 0.00433825\n",
      "Iteration 291, loss = 0.00421574\n",
      "Iteration 292, loss = 0.00399994\n",
      "Iteration 293, loss = 0.00400545\n",
      "Iteration 294, loss = 0.00409557\n",
      "Iteration 295, loss = 0.00404742\n",
      "Iteration 296, loss = 0.00375410\n",
      "Iteration 297, loss = 0.00397158\n",
      "Iteration 298, loss = 0.00398195\n",
      "Iteration 299, loss = 0.00409836\n",
      "Iteration 300, loss = 0.00391216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=300, alpha=0.0001,\n",
    "                    solver='adam', verbose=10, random_state=42, tol=0.000000001)\n",
    "\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.90732636\n",
      "Iteration 2, loss = 2.68733847\n",
      "Iteration 3, loss = 2.46371642\n",
      "Iteration 4, loss = 2.22323778\n",
      "Iteration 5, loss = 1.97130815\n",
      "Iteration 6, loss = 1.70612073\n",
      "Iteration 7, loss = 1.44034146\n",
      "Iteration 8, loss = 1.18726092\n",
      "Iteration 9, loss = 0.96228794\n",
      "Iteration 10, loss = 0.78244650\n",
      "Iteration 11, loss = 0.64295161\n",
      "Iteration 12, loss = 0.53732636\n",
      "Iteration 13, loss = 0.45618164\n",
      "Iteration 14, loss = 0.39471466\n",
      "Iteration 15, loss = 0.34527210\n",
      "Iteration 16, loss = 0.30742882\n",
      "Iteration 17, loss = 0.27806668\n",
      "Iteration 18, loss = 0.25273405\n",
      "Iteration 19, loss = 0.23266842\n",
      "Iteration 20, loss = 0.21441396\n",
      "Iteration 21, loss = 0.20071983\n",
      "Iteration 22, loss = 0.18647346\n",
      "Iteration 23, loss = 0.17572211\n",
      "Iteration 24, loss = 0.16529245\n",
      "Iteration 25, loss = 0.15660575\n",
      "Iteration 26, loss = 0.14754489\n",
      "Iteration 27, loss = 0.14001165\n",
      "Iteration 28, loss = 0.13382421\n",
      "Iteration 29, loss = 0.12781526\n",
      "Iteration 30, loss = 0.12374396\n",
      "Iteration 31, loss = 0.11667979\n",
      "Iteration 32, loss = 0.11319013\n",
      "Iteration 33, loss = 0.10793913\n",
      "Iteration 34, loss = 0.10455204\n",
      "Iteration 35, loss = 0.10048049\n",
      "Iteration 36, loss = 0.09740200\n",
      "Iteration 37, loss = 0.09538948\n",
      "Iteration 38, loss = 0.09169315\n",
      "Iteration 39, loss = 0.08922848\n",
      "Iteration 40, loss = 0.08555889\n",
      "Iteration 41, loss = 0.08391282\n",
      "Iteration 42, loss = 0.08089561\n",
      "Iteration 43, loss = 0.07901994\n",
      "Iteration 44, loss = 0.07736361\n",
      "Iteration 45, loss = 0.07536363\n",
      "Iteration 46, loss = 0.07299968\n",
      "Iteration 47, loss = 0.07275604\n",
      "Iteration 48, loss = 0.07032698\n",
      "Iteration 49, loss = 0.06800349\n",
      "Iteration 50, loss = 0.06739705\n",
      "Iteration 51, loss = 0.06515209\n",
      "Iteration 52, loss = 0.06427423\n",
      "Iteration 53, loss = 0.06254339\n",
      "Iteration 54, loss = 0.06044514\n",
      "Iteration 55, loss = 0.06000369\n",
      "Iteration 56, loss = 0.05769878\n",
      "Iteration 57, loss = 0.05750931\n",
      "Iteration 58, loss = 0.05528644\n",
      "Iteration 59, loss = 0.05444798\n",
      "Iteration 60, loss = 0.05397276\n",
      "Iteration 61, loss = 0.05295890\n",
      "Iteration 62, loss = 0.05155794\n",
      "Iteration 63, loss = 0.05028299\n",
      "Iteration 64, loss = 0.04976208\n",
      "Iteration 65, loss = 0.04858396\n",
      "Iteration 66, loss = 0.04771935\n",
      "Iteration 67, loss = 0.04703252\n",
      "Iteration 68, loss = 0.04597521\n",
      "Iteration 69, loss = 0.04542507\n",
      "Iteration 70, loss = 0.04392714\n",
      "Iteration 71, loss = 0.04378492\n",
      "Iteration 72, loss = 0.04327461\n",
      "Iteration 73, loss = 0.04178402\n",
      "Iteration 74, loss = 0.04093764\n",
      "Iteration 75, loss = 0.04020429\n",
      "Iteration 76, loss = 0.03922431\n",
      "Iteration 77, loss = 0.03914888\n",
      "Iteration 78, loss = 0.03842337\n",
      "Iteration 79, loss = 0.03761559\n",
      "Iteration 80, loss = 0.03703026\n",
      "Iteration 81, loss = 0.03654002\n",
      "Iteration 82, loss = 0.03591212\n",
      "Iteration 83, loss = 0.03565370\n",
      "Iteration 84, loss = 0.03475553\n",
      "Iteration 85, loss = 0.03501105\n",
      "Iteration 86, loss = 0.03356830\n",
      "Iteration 87, loss = 0.03295282\n",
      "Iteration 88, loss = 0.03294809\n",
      "Iteration 89, loss = 0.03173947\n",
      "Iteration 90, loss = 0.03163961\n",
      "Iteration 91, loss = 0.03101100\n",
      "Iteration 92, loss = 0.03062289\n",
      "Iteration 93, loss = 0.03025709\n",
      "Iteration 94, loss = 0.02983060\n",
      "Iteration 95, loss = 0.02920180\n",
      "Iteration 96, loss = 0.02873645\n",
      "Iteration 97, loss = 0.02899797\n",
      "Iteration 98, loss = 0.02775790\n",
      "Iteration 99, loss = 0.02749180\n",
      "Iteration 100, loss = 0.02687309\n",
      "Iteration 101, loss = 0.02665795\n",
      "Iteration 102, loss = 0.02623669\n",
      "Iteration 103, loss = 0.02566364\n",
      "Iteration 104, loss = 0.02614419\n",
      "Iteration 105, loss = 0.02532520\n",
      "Iteration 106, loss = 0.02666636\n",
      "Iteration 107, loss = 0.02471446\n",
      "Iteration 108, loss = 0.02375312\n",
      "Iteration 109, loss = 0.02434844\n",
      "Iteration 110, loss = 0.02405185\n",
      "Iteration 111, loss = 0.02344162\n",
      "Iteration 112, loss = 0.02299265\n",
      "Iteration 113, loss = 0.02243685\n",
      "Iteration 114, loss = 0.02233525\n",
      "Iteration 115, loss = 0.02162662\n",
      "Iteration 116, loss = 0.02164772\n",
      "Iteration 117, loss = 0.02151182\n",
      "Iteration 118, loss = 0.02046885\n",
      "Iteration 119, loss = 0.02080223\n",
      "Iteration 120, loss = 0.02059376\n",
      "Iteration 121, loss = 0.01986912\n",
      "Iteration 122, loss = 0.02027794\n",
      "Iteration 123, loss = 0.01933677\n",
      "Iteration 124, loss = 0.01942883\n",
      "Iteration 125, loss = 0.02063857\n",
      "Iteration 126, loss = 0.02063829\n",
      "Iteration 127, loss = 0.02013609\n",
      "Iteration 128, loss = 0.01836544\n",
      "Iteration 129, loss = 0.01817993\n",
      "Iteration 130, loss = 0.01855548\n",
      "Iteration 131, loss = 0.01771562\n",
      "Iteration 132, loss = 0.01727339\n",
      "Iteration 133, loss = 0.01734393\n",
      "Iteration 134, loss = 0.01708138\n",
      "Iteration 135, loss = 0.01661600\n",
      "Iteration 136, loss = 0.01673183\n",
      "Iteration 137, loss = 0.01659454\n",
      "Iteration 138, loss = 0.01609404\n",
      "Iteration 139, loss = 0.01560027\n",
      "Iteration 140, loss = 0.01578517\n",
      "Iteration 141, loss = 0.01561358\n",
      "Iteration 142, loss = 0.01558562\n",
      "Iteration 143, loss = 0.01588710\n",
      "Iteration 144, loss = 0.01516349\n",
      "Iteration 145, loss = 0.01536020\n",
      "Iteration 146, loss = 0.01462210\n",
      "Iteration 147, loss = 0.01433565\n",
      "Iteration 148, loss = 0.01438016\n",
      "Iteration 149, loss = 0.01428858\n",
      "Iteration 150, loss = 0.01394439\n",
      "Iteration 151, loss = 0.01428802\n",
      "Iteration 152, loss = 0.01351992\n",
      "Iteration 153, loss = 0.01361415\n",
      "Iteration 154, loss = 0.01325113\n",
      "Iteration 155, loss = 0.01313249\n",
      "Iteration 156, loss = 0.01312722\n",
      "Iteration 157, loss = 0.01389589\n",
      "Iteration 158, loss = 0.01245154\n",
      "Iteration 159, loss = 0.01279550\n",
      "Iteration 160, loss = 0.01381033\n",
      "Iteration 161, loss = 0.01454093\n",
      "Iteration 162, loss = 0.01401162\n",
      "Iteration 163, loss = 0.01178684\n",
      "Iteration 164, loss = 0.01220767\n",
      "Iteration 165, loss = 0.01182300\n",
      "Iteration 166, loss = 0.01173310\n",
      "Iteration 167, loss = 0.01131584\n",
      "Iteration 168, loss = 0.01145676\n",
      "Iteration 169, loss = 0.01166593\n",
      "Iteration 170, loss = 0.01153158\n",
      "Iteration 171, loss = 0.01128199\n",
      "Iteration 172, loss = 0.01137341\n",
      "Iteration 173, loss = 0.01076060\n",
      "Iteration 174, loss = 0.01073168\n",
      "Iteration 175, loss = 0.01072302\n",
      "Iteration 176, loss = 0.01097045\n",
      "Iteration 177, loss = 0.01038975\n",
      "Iteration 178, loss = 0.01020785\n",
      "Iteration 179, loss = 0.01041712\n",
      "Iteration 180, loss = 0.01017367\n",
      "Iteration 181, loss = 0.00981368\n",
      "Iteration 182, loss = 0.00988032\n",
      "Iteration 183, loss = 0.00987379\n",
      "Iteration 184, loss = 0.00981238\n",
      "Iteration 185, loss = 0.00944041\n",
      "Iteration 186, loss = 0.00970427\n",
      "Iteration 187, loss = 0.00938845\n",
      "Iteration 188, loss = 0.00931585\n",
      "Iteration 189, loss = 0.00918765\n",
      "Iteration 190, loss = 0.00914210\n",
      "Iteration 191, loss = 0.00912463\n",
      "Iteration 192, loss = 0.00882140\n",
      "Iteration 193, loss = 0.00884231\n",
      "Iteration 194, loss = 0.00937619\n",
      "Iteration 195, loss = 0.00977284\n",
      "Iteration 196, loss = 0.00861870\n",
      "Iteration 197, loss = 0.00828909\n",
      "Iteration 198, loss = 0.00859552\n",
      "Iteration 199, loss = 0.00849276\n",
      "Iteration 200, loss = 0.00799123\n",
      "Iteration 201, loss = 0.00843421\n",
      "Iteration 202, loss = 0.00851623\n",
      "Iteration 203, loss = 0.00804662\n",
      "Iteration 204, loss = 0.00784429\n",
      "Iteration 205, loss = 0.00778535\n",
      "Iteration 206, loss = 0.00767737\n",
      "Iteration 207, loss = 0.00748196\n",
      "Iteration 208, loss = 0.00785904\n",
      "Iteration 209, loss = 0.00777274\n",
      "Iteration 210, loss = 0.00779981\n",
      "Iteration 211, loss = 0.00732358\n",
      "Iteration 212, loss = 0.00737237\n",
      "Iteration 213, loss = 0.00721203\n",
      "Iteration 214, loss = 0.00784966\n",
      "Iteration 215, loss = 0.00707797\n",
      "Iteration 216, loss = 0.00682808\n",
      "Iteration 217, loss = 0.00699992\n",
      "Iteration 218, loss = 0.00668762\n",
      "Iteration 219, loss = 0.00710262\n",
      "Iteration 220, loss = 0.00691201\n",
      "Iteration 221, loss = 0.00656984\n",
      "Iteration 222, loss = 0.00656480\n",
      "Iteration 223, loss = 0.00655307\n",
      "Iteration 224, loss = 0.00637390\n",
      "Iteration 225, loss = 0.00633764\n",
      "Iteration 226, loss = 0.00665543\n",
      "Iteration 227, loss = 0.00617753\n",
      "Iteration 228, loss = 0.00614277\n",
      "Iteration 229, loss = 0.00603859\n",
      "Iteration 230, loss = 0.00597823\n",
      "Iteration 231, loss = 0.00601206\n",
      "Iteration 232, loss = 0.00629760\n",
      "Iteration 233, loss = 0.00649073\n",
      "Iteration 234, loss = 0.00583152\n",
      "Iteration 235, loss = 0.00585017\n",
      "Iteration 236, loss = 0.00578743\n",
      "Iteration 237, loss = 0.00572088\n",
      "Iteration 238, loss = 0.00547583\n",
      "Iteration 239, loss = 0.00551369\n",
      "Iteration 240, loss = 0.00541836\n",
      "Iteration 241, loss = 0.00549285\n",
      "Iteration 242, loss = 0.00541696\n",
      "Iteration 243, loss = 0.00527702\n",
      "Iteration 244, loss = 0.00529691\n",
      "Iteration 245, loss = 0.00521383\n",
      "Iteration 246, loss = 0.00528608\n",
      "Iteration 247, loss = 0.00562631\n",
      "Iteration 248, loss = 0.00605467\n",
      "Iteration 249, loss = 0.00630987\n",
      "Iteration 250, loss = 0.00666723\n",
      "Iteration 251, loss = 0.00705005\n",
      "Iteration 252, loss = 0.00608849\n",
      "Iteration 253, loss = 0.00512690\n",
      "Iteration 254, loss = 0.00535289\n",
      "Iteration 255, loss = 0.00574804\n",
      "Iteration 256, loss = 0.00487455\n",
      "Iteration 257, loss = 0.00462820\n",
      "Iteration 258, loss = 0.00454528\n",
      "Iteration 259, loss = 0.00457542\n",
      "Iteration 260, loss = 0.00443587\n",
      "Iteration 261, loss = 0.00459579\n",
      "Iteration 262, loss = 0.00465616\n",
      "Iteration 263, loss = 0.00440983\n",
      "Iteration 264, loss = 0.00429082\n",
      "Iteration 265, loss = 0.00434716\n",
      "Iteration 266, loss = 0.00429309\n",
      "Iteration 267, loss = 0.00510406\n",
      "Iteration 268, loss = 0.00491524\n",
      "Iteration 269, loss = 0.00449358\n",
      "Iteration 270, loss = 0.00415724\n",
      "Iteration 271, loss = 0.00432152\n",
      "Iteration 272, loss = 0.00486318\n",
      "Iteration 273, loss = 0.00418365\n",
      "Iteration 274, loss = 0.00412124\n",
      "Iteration 275, loss = 0.00393157\n",
      "Iteration 276, loss = 0.00404448\n",
      "Iteration 277, loss = 0.00383502\n",
      "Iteration 278, loss = 0.00383514\n",
      "Iteration 279, loss = 0.00391743\n",
      "Iteration 280, loss = 0.00433087\n",
      "Iteration 281, loss = 0.00432756\n",
      "Iteration 282, loss = 0.00375191\n",
      "Iteration 283, loss = 0.00395079\n",
      "Iteration 284, loss = 0.00381412\n",
      "Iteration 285, loss = 0.00360115\n",
      "Iteration 286, loss = 0.00351432\n",
      "Iteration 287, loss = 0.00353953\n",
      "Iteration 288, loss = 0.00364429\n",
      "Iteration 289, loss = 0.00366201\n",
      "Iteration 290, loss = 0.00331137\n",
      "Iteration 291, loss = 0.00356676\n",
      "Iteration 292, loss = 0.00367061\n",
      "Iteration 293, loss = 0.00330251\n",
      "Iteration 294, loss = 0.00336083\n",
      "Iteration 295, loss = 0.00345520\n",
      "Iteration 296, loss = 0.00342130\n",
      "Iteration 297, loss = 0.00316941\n",
      "Iteration 298, loss = 0.00315567\n",
      "Iteration 299, loss = 0.00314031\n",
      "Iteration 300, loss = 0.00307247\n",
      "Iteration 1, loss = 2.90886697\n",
      "Iteration 2, loss = 2.69161771\n",
      "Iteration 3, loss = 2.47094912\n",
      "Iteration 4, loss = 2.23293348\n",
      "Iteration 5, loss = 1.97888811\n",
      "Iteration 6, loss = 1.71407869\n",
      "Iteration 7, loss = 1.44475539\n",
      "Iteration 8, loss = 1.18933549\n",
      "Iteration 9, loss = 0.96210226\n",
      "Iteration 10, loss = 0.78138913\n",
      "Iteration 11, loss = 0.63971240\n",
      "Iteration 12, loss = 0.53395832\n",
      "Iteration 13, loss = 0.45357912\n",
      "Iteration 14, loss = 0.39443891\n",
      "Iteration 15, loss = 0.34702731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.31037139\n",
      "Iteration 17, loss = 0.28243030\n",
      "Iteration 18, loss = 0.25697384\n",
      "Iteration 19, loss = 0.23792106\n",
      "Iteration 20, loss = 0.22043246\n",
      "Iteration 21, loss = 0.20511117\n",
      "Iteration 22, loss = 0.19219876\n",
      "Iteration 23, loss = 0.18095171\n",
      "Iteration 24, loss = 0.17014687\n",
      "Iteration 25, loss = 0.16155756\n",
      "Iteration 26, loss = 0.15336977\n",
      "Iteration 27, loss = 0.14551694\n",
      "Iteration 28, loss = 0.13936231\n",
      "Iteration 29, loss = 0.13354037\n",
      "Iteration 30, loss = 0.12786757\n",
      "Iteration 31, loss = 0.12252401\n",
      "Iteration 32, loss = 0.11807769\n",
      "Iteration 33, loss = 0.11393070\n",
      "Iteration 34, loss = 0.10975998\n",
      "Iteration 35, loss = 0.10712178\n",
      "Iteration 36, loss = 0.10297301\n",
      "Iteration 37, loss = 0.09972071\n",
      "Iteration 38, loss = 0.09692425\n",
      "Iteration 39, loss = 0.09423920\n",
      "Iteration 40, loss = 0.09150465\n",
      "Iteration 41, loss = 0.08897173\n",
      "Iteration 42, loss = 0.08594001\n",
      "Iteration 43, loss = 0.08371795\n",
      "Iteration 44, loss = 0.08187244\n",
      "Iteration 45, loss = 0.08033397\n",
      "Iteration 46, loss = 0.07761567\n",
      "Iteration 47, loss = 0.07682513\n",
      "Iteration 48, loss = 0.07442615\n",
      "Iteration 49, loss = 0.07281161\n",
      "Iteration 50, loss = 0.07173170\n",
      "Iteration 51, loss = 0.07041853\n",
      "Iteration 52, loss = 0.06929703\n",
      "Iteration 53, loss = 0.06723979\n",
      "Iteration 54, loss = 0.06449877\n",
      "Iteration 55, loss = 0.06419750\n",
      "Iteration 56, loss = 0.06114153\n",
      "Iteration 57, loss = 0.06157562\n",
      "Iteration 58, loss = 0.05923050\n",
      "Iteration 59, loss = 0.05844545\n",
      "Iteration 60, loss = 0.05772730\n",
      "Iteration 61, loss = 0.05605254\n",
      "Iteration 62, loss = 0.05458185\n",
      "Iteration 63, loss = 0.05339998\n",
      "Iteration 64, loss = 0.05284270\n",
      "Iteration 65, loss = 0.05144614\n",
      "Iteration 66, loss = 0.05140826\n",
      "Iteration 67, loss = 0.04999630\n",
      "Iteration 68, loss = 0.04836719\n",
      "Iteration 69, loss = 0.04722717\n",
      "Iteration 70, loss = 0.04710839\n",
      "Iteration 71, loss = 0.04718687\n",
      "Iteration 72, loss = 0.04477303\n",
      "Iteration 73, loss = 0.04420547\n",
      "Iteration 74, loss = 0.04320579\n",
      "Iteration 75, loss = 0.04211413\n",
      "Iteration 76, loss = 0.04118206\n",
      "Iteration 77, loss = 0.04041949\n",
      "Iteration 78, loss = 0.03983664\n",
      "Iteration 79, loss = 0.03921598\n",
      "Iteration 80, loss = 0.03920049\n",
      "Iteration 81, loss = 0.03908093\n",
      "Iteration 82, loss = 0.03734501\n",
      "Iteration 83, loss = 0.03633267\n",
      "Iteration 84, loss = 0.03609034\n",
      "Iteration 85, loss = 0.03564345\n",
      "Iteration 86, loss = 0.03489098\n",
      "Iteration 87, loss = 0.03447039\n",
      "Iteration 88, loss = 0.03353857\n",
      "Iteration 89, loss = 0.03302433\n",
      "Iteration 90, loss = 0.03240313\n",
      "Iteration 91, loss = 0.03236994\n",
      "Iteration 92, loss = 0.03119298\n",
      "Iteration 93, loss = 0.03086109\n",
      "Iteration 94, loss = 0.03055716\n",
      "Iteration 95, loss = 0.03004855\n",
      "Iteration 96, loss = 0.02990274\n",
      "Iteration 97, loss = 0.02871952\n",
      "Iteration 98, loss = 0.02828671\n",
      "Iteration 99, loss = 0.02800829\n",
      "Iteration 100, loss = 0.02730656\n",
      "Iteration 101, loss = 0.02763889\n",
      "Iteration 102, loss = 0.02682562\n",
      "Iteration 103, loss = 0.02627132\n",
      "Iteration 104, loss = 0.02558930\n",
      "Iteration 105, loss = 0.02576804\n",
      "Iteration 106, loss = 0.02594860\n",
      "Iteration 107, loss = 0.02520860\n",
      "Iteration 108, loss = 0.02428522\n",
      "Iteration 109, loss = 0.02442082\n",
      "Iteration 110, loss = 0.02366283\n",
      "Iteration 111, loss = 0.02300748\n",
      "Iteration 112, loss = 0.02284928\n",
      "Iteration 113, loss = 0.02235621\n",
      "Iteration 114, loss = 0.02227423\n",
      "Iteration 115, loss = 0.02207271\n",
      "Iteration 116, loss = 0.02139739\n",
      "Iteration 117, loss = 0.02151347\n",
      "Iteration 118, loss = 0.02053417\n",
      "Iteration 119, loss = 0.02085463\n",
      "Iteration 120, loss = 0.02040214\n",
      "Iteration 121, loss = 0.01993914\n",
      "Iteration 122, loss = 0.02160857\n",
      "Iteration 123, loss = 0.02075190\n",
      "Iteration 124, loss = 0.01966193\n",
      "Iteration 125, loss = 0.01945508\n",
      "Iteration 126, loss = 0.01977439\n",
      "Iteration 127, loss = 0.02042802\n",
      "Iteration 128, loss = 0.01947223\n",
      "Iteration 129, loss = 0.01888675\n",
      "Iteration 130, loss = 0.01842301\n",
      "Iteration 131, loss = 0.01729332\n",
      "Iteration 132, loss = 0.01766862\n",
      "Iteration 133, loss = 0.01716246\n",
      "Iteration 134, loss = 0.01683274\n",
      "Iteration 135, loss = 0.01676256\n",
      "Iteration 136, loss = 0.01628762\n",
      "Iteration 137, loss = 0.01619679\n",
      "Iteration 138, loss = 0.01595870\n",
      "Iteration 139, loss = 0.01623679\n",
      "Iteration 140, loss = 0.01629842\n",
      "Iteration 141, loss = 0.01556110\n",
      "Iteration 142, loss = 0.01568029\n",
      "Iteration 143, loss = 0.01597713\n",
      "Iteration 144, loss = 0.01493172\n",
      "Iteration 145, loss = 0.01589705\n",
      "Iteration 146, loss = 0.01512411\n",
      "Iteration 147, loss = 0.01405827\n",
      "Iteration 148, loss = 0.01480755\n",
      "Iteration 149, loss = 0.01411010\n",
      "Iteration 150, loss = 0.01425844\n",
      "Iteration 151, loss = 0.01373014\n",
      "Iteration 152, loss = 0.01357013\n",
      "Iteration 153, loss = 0.01324979\n",
      "Iteration 154, loss = 0.01309581\n",
      "Iteration 155, loss = 0.01309370\n",
      "Iteration 156, loss = 0.01284129\n",
      "Iteration 157, loss = 0.01323510\n",
      "Iteration 158, loss = 0.01243335\n",
      "Iteration 159, loss = 0.01240156\n",
      "Iteration 160, loss = 0.01310294\n",
      "Iteration 161, loss = 0.01476084\n",
      "Iteration 162, loss = 0.01406355\n",
      "Iteration 163, loss = 0.01187893\n",
      "Iteration 164, loss = 0.01161126\n",
      "Iteration 165, loss = 0.01219448\n",
      "Iteration 166, loss = 0.01281195\n",
      "Iteration 167, loss = 0.01154121\n",
      "Iteration 168, loss = 0.01154431\n",
      "Iteration 169, loss = 0.01104722\n",
      "Iteration 170, loss = 0.01097335\n",
      "Iteration 171, loss = 0.01088599\n",
      "Iteration 172, loss = 0.01139720\n",
      "Iteration 173, loss = 0.01103394\n",
      "Iteration 174, loss = 0.01040583\n",
      "Iteration 175, loss = 0.01040792\n",
      "Iteration 176, loss = 0.01056420\n",
      "Iteration 177, loss = 0.01022525\n",
      "Iteration 178, loss = 0.01017756\n",
      "Iteration 179, loss = 0.01060017\n",
      "Iteration 180, loss = 0.01128657\n",
      "Iteration 181, loss = 0.01115474\n",
      "Iteration 182, loss = 0.01104279\n",
      "Iteration 183, loss = 0.01050638\n",
      "Iteration 184, loss = 0.00987767\n",
      "Iteration 185, loss = 0.00939520\n",
      "Iteration 186, loss = 0.00932424\n",
      "Iteration 187, loss = 0.00940256\n",
      "Iteration 188, loss = 0.00891039\n",
      "Iteration 189, loss = 0.00927138\n",
      "Iteration 190, loss = 0.00881777\n",
      "Iteration 191, loss = 0.00870232\n",
      "Iteration 192, loss = 0.00881029\n",
      "Iteration 193, loss = 0.00886209\n",
      "Iteration 194, loss = 0.00896949\n",
      "Iteration 195, loss = 0.00890061\n",
      "Iteration 196, loss = 0.00827871\n",
      "Iteration 197, loss = 0.00804501\n",
      "Iteration 198, loss = 0.00835386\n",
      "Iteration 199, loss = 0.00846410\n",
      "Iteration 200, loss = 0.00794148\n",
      "Iteration 201, loss = 0.00753075\n",
      "Iteration 202, loss = 0.00834562\n",
      "Iteration 203, loss = 0.00816486\n",
      "Iteration 204, loss = 0.00781901\n",
      "Iteration 205, loss = 0.00751558\n",
      "Iteration 206, loss = 0.00770445\n",
      "Iteration 207, loss = 0.00722137\n",
      "Iteration 208, loss = 0.00733611\n",
      "Iteration 209, loss = 0.00727537\n",
      "Iteration 210, loss = 0.00765585\n",
      "Iteration 211, loss = 0.00744724\n",
      "Iteration 212, loss = 0.00685274\n",
      "Iteration 213, loss = 0.00693159\n",
      "Iteration 214, loss = 0.00713702\n",
      "Iteration 215, loss = 0.00680446\n",
      "Iteration 216, loss = 0.00674020\n",
      "Iteration 217, loss = 0.00664637\n",
      "Iteration 218, loss = 0.00673881\n",
      "Iteration 219, loss = 0.00674685\n",
      "Iteration 220, loss = 0.00666241\n",
      "Iteration 221, loss = 0.00635483\n",
      "Iteration 222, loss = 0.00642188\n",
      "Iteration 223, loss = 0.00622125\n",
      "Iteration 224, loss = 0.00630302\n",
      "Iteration 225, loss = 0.00635137\n",
      "Iteration 226, loss = 0.00623183\n",
      "Iteration 227, loss = 0.00582797\n",
      "Iteration 228, loss = 0.00611713\n",
      "Iteration 229, loss = 0.00579197\n",
      "Iteration 230, loss = 0.00574580\n",
      "Iteration 231, loss = 0.00572712\n",
      "Iteration 232, loss = 0.00585386\n",
      "Iteration 233, loss = 0.00602978\n",
      "Iteration 234, loss = 0.00592564\n",
      "Iteration 235, loss = 0.00532184\n",
      "Iteration 236, loss = 0.00589703\n",
      "Iteration 237, loss = 0.00533266\n",
      "Iteration 238, loss = 0.00537363\n",
      "Iteration 239, loss = 0.00533799\n",
      "Iteration 240, loss = 0.00522323\n",
      "Iteration 241, loss = 0.00535629\n",
      "Iteration 242, loss = 0.00518250\n",
      "Iteration 243, loss = 0.00533585\n",
      "Iteration 244, loss = 0.00500861\n",
      "Iteration 245, loss = 0.00502350\n",
      "Iteration 246, loss = 0.00489245\n",
      "Iteration 247, loss = 0.00559634\n",
      "Iteration 248, loss = 0.00655840\n",
      "Iteration 249, loss = 0.00752478\n",
      "Iteration 250, loss = 0.00886113\n",
      "Iteration 251, loss = 0.00990166\n",
      "Iteration 252, loss = 0.00487079\n",
      "Iteration 253, loss = 0.00568735\n",
      "Iteration 254, loss = 0.00451760\n",
      "Iteration 255, loss = 0.00549313\n",
      "Iteration 256, loss = 0.00492774\n",
      "Iteration 257, loss = 0.00448219\n",
      "Iteration 258, loss = 0.00445436\n",
      "Iteration 259, loss = 0.00442992\n",
      "Iteration 260, loss = 0.00433994\n",
      "Iteration 261, loss = 0.00438831\n",
      "Iteration 262, loss = 0.00416958\n",
      "Iteration 263, loss = 0.00413117\n",
      "Iteration 264, loss = 0.00422223\n",
      "Iteration 265, loss = 0.00431197\n",
      "Iteration 266, loss = 0.00407496\n",
      "Iteration 267, loss = 0.00436504\n",
      "Iteration 268, loss = 0.00398949\n",
      "Iteration 269, loss = 0.00407084\n",
      "Iteration 270, loss = 0.00384848\n",
      "Iteration 271, loss = 0.00395219\n",
      "Iteration 272, loss = 0.00407154\n",
      "Iteration 273, loss = 0.00383517\n",
      "Iteration 274, loss = 0.00382629\n",
      "Iteration 275, loss = 0.00369040\n",
      "Iteration 276, loss = 0.00373391\n",
      "Iteration 277, loss = 0.00357396\n",
      "Iteration 278, loss = 0.00369475\n",
      "Iteration 279, loss = 0.00368729\n",
      "Iteration 280, loss = 0.00384498\n",
      "Iteration 281, loss = 0.00388495\n",
      "Iteration 282, loss = 0.00340669\n",
      "Iteration 283, loss = 0.00391043\n",
      "Iteration 284, loss = 0.00379224\n",
      "Iteration 285, loss = 0.00345239\n",
      "Iteration 286, loss = 0.00341402\n",
      "Iteration 287, loss = 0.00338940\n",
      "Iteration 288, loss = 0.00342659\n",
      "Iteration 289, loss = 0.00351907\n",
      "Iteration 290, loss = 0.00326928\n",
      "Iteration 291, loss = 0.00332802\n",
      "Iteration 292, loss = 0.00328840\n",
      "Iteration 293, loss = 0.00318363\n",
      "Iteration 294, loss = 0.00318332\n",
      "Iteration 295, loss = 0.00334486\n",
      "Iteration 296, loss = 0.00330854\n",
      "Iteration 297, loss = 0.00303917\n",
      "Iteration 298, loss = 0.00312166\n",
      "Iteration 299, loss = 0.00308863\n",
      "Iteration 300, loss = 0.00297017\n",
      "Iteration 1, loss = 2.90637443\n",
      "Iteration 2, loss = 2.68772329\n",
      "Iteration 3, loss = 2.46562678\n",
      "Iteration 4, loss = 2.22380616\n",
      "Iteration 5, loss = 1.97197024\n",
      "Iteration 6, loss = 1.70782687\n",
      "Iteration 7, loss = 1.43819576\n",
      "Iteration 8, loss = 1.18402394\n",
      "Iteration 9, loss = 0.96028761\n",
      "Iteration 10, loss = 0.77955038\n",
      "Iteration 11, loss = 0.63917156\n",
      "Iteration 12, loss = 0.53432291\n",
      "Iteration 13, loss = 0.45410446\n",
      "Iteration 14, loss = 0.39341639\n",
      "Iteration 15, loss = 0.34494183\n",
      "Iteration 16, loss = 0.30785245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.27745776\n",
      "Iteration 18, loss = 0.25254681\n",
      "Iteration 19, loss = 0.23260550\n",
      "Iteration 20, loss = 0.21533714\n",
      "Iteration 21, loss = 0.20065659\n",
      "Iteration 22, loss = 0.18797245\n",
      "Iteration 23, loss = 0.17682646\n",
      "Iteration 24, loss = 0.16706976\n",
      "Iteration 25, loss = 0.15867662\n",
      "Iteration 26, loss = 0.15130668\n",
      "Iteration 27, loss = 0.14388410\n",
      "Iteration 28, loss = 0.13754704\n",
      "Iteration 29, loss = 0.13179468\n",
      "Iteration 30, loss = 0.12633945\n",
      "Iteration 31, loss = 0.12133492\n",
      "Iteration 32, loss = 0.11783672\n",
      "Iteration 33, loss = 0.11296253\n",
      "Iteration 34, loss = 0.10991312\n",
      "Iteration 35, loss = 0.10665254\n",
      "Iteration 36, loss = 0.10263120\n",
      "Iteration 37, loss = 0.09935544\n",
      "Iteration 38, loss = 0.09658872\n",
      "Iteration 39, loss = 0.09415147\n",
      "Iteration 40, loss = 0.09237500\n",
      "Iteration 41, loss = 0.08824245\n",
      "Iteration 42, loss = 0.08655739\n",
      "Iteration 43, loss = 0.08441802\n",
      "Iteration 44, loss = 0.08300256\n",
      "Iteration 45, loss = 0.07970849\n",
      "Iteration 46, loss = 0.07889533\n",
      "Iteration 47, loss = 0.07564983\n",
      "Iteration 48, loss = 0.07545382\n",
      "Iteration 49, loss = 0.07413535\n",
      "Iteration 50, loss = 0.07179346\n",
      "Iteration 51, loss = 0.07008663\n",
      "Iteration 52, loss = 0.06832605\n",
      "Iteration 53, loss = 0.06708354\n",
      "Iteration 54, loss = 0.06525484\n",
      "Iteration 55, loss = 0.06592938\n",
      "Iteration 56, loss = 0.06280391\n",
      "Iteration 57, loss = 0.06204608\n",
      "Iteration 58, loss = 0.06082863\n",
      "Iteration 59, loss = 0.06020125\n",
      "Iteration 60, loss = 0.05875964\n",
      "Iteration 61, loss = 0.05752920\n",
      "Iteration 62, loss = 0.05609009\n",
      "Iteration 63, loss = 0.05552446\n",
      "Iteration 64, loss = 0.05409910\n",
      "Iteration 65, loss = 0.05349449\n",
      "Iteration 66, loss = 0.05231064\n",
      "Iteration 67, loss = 0.05192038\n",
      "Iteration 68, loss = 0.05125660\n",
      "Iteration 69, loss = 0.04929781\n",
      "Iteration 70, loss = 0.04921912\n",
      "Iteration 71, loss = 0.04834669\n",
      "Iteration 72, loss = 0.04735069\n",
      "Iteration 73, loss = 0.04690583\n",
      "Iteration 74, loss = 0.04615273\n",
      "Iteration 75, loss = 0.04493457\n",
      "Iteration 76, loss = 0.04415458\n",
      "Iteration 77, loss = 0.04345424\n",
      "Iteration 78, loss = 0.04293300\n",
      "Iteration 79, loss = 0.04200661\n",
      "Iteration 80, loss = 0.04163081\n",
      "Iteration 81, loss = 0.04067204\n",
      "Iteration 82, loss = 0.04021235\n",
      "Iteration 83, loss = 0.04026140\n",
      "Iteration 84, loss = 0.03910463\n",
      "Iteration 85, loss = 0.03828197\n",
      "Iteration 86, loss = 0.03805581\n",
      "Iteration 87, loss = 0.03770595\n",
      "Iteration 88, loss = 0.03659469\n",
      "Iteration 89, loss = 0.03638635\n",
      "Iteration 90, loss = 0.03557596\n",
      "Iteration 91, loss = 0.03482955\n",
      "Iteration 92, loss = 0.03517937\n",
      "Iteration 93, loss = 0.03371185\n",
      "Iteration 94, loss = 0.03346183\n",
      "Iteration 95, loss = 0.03336489\n",
      "Iteration 96, loss = 0.03297377\n",
      "Iteration 97, loss = 0.03155624\n",
      "Iteration 98, loss = 0.03224732\n",
      "Iteration 99, loss = 0.03071537\n",
      "Iteration 100, loss = 0.03105177\n",
      "Iteration 101, loss = 0.03007592\n",
      "Iteration 102, loss = 0.02967923\n",
      "Iteration 103, loss = 0.02927197\n",
      "Iteration 104, loss = 0.02909667\n",
      "Iteration 105, loss = 0.02850289\n",
      "Iteration 106, loss = 0.02966773\n",
      "Iteration 107, loss = 0.02901260\n",
      "Iteration 108, loss = 0.02869100\n",
      "Iteration 109, loss = 0.02703599\n",
      "Iteration 110, loss = 0.02597453\n",
      "Iteration 111, loss = 0.02693736\n",
      "Iteration 112, loss = 0.02690955\n",
      "Iteration 113, loss = 0.02550882\n",
      "Iteration 114, loss = 0.02512971\n",
      "Iteration 115, loss = 0.02443694\n",
      "Iteration 116, loss = 0.02396277\n",
      "Iteration 117, loss = 0.02368840\n",
      "Iteration 118, loss = 0.02400942\n",
      "Iteration 119, loss = 0.02352919\n",
      "Iteration 120, loss = 0.02286025\n",
      "Iteration 121, loss = 0.02264851\n",
      "Iteration 122, loss = 0.02238109\n",
      "Iteration 123, loss = 0.02204460\n",
      "Iteration 124, loss = 0.02210612\n",
      "Iteration 125, loss = 0.02134150\n",
      "Iteration 126, loss = 0.02111143\n",
      "Iteration 127, loss = 0.02064773\n",
      "Iteration 128, loss = 0.02043709\n",
      "Iteration 129, loss = 0.02040809\n",
      "Iteration 130, loss = 0.02053973\n",
      "Iteration 131, loss = 0.01977898\n",
      "Iteration 132, loss = 0.01956045\n",
      "Iteration 133, loss = 0.01942089\n",
      "Iteration 134, loss = 0.01905537\n",
      "Iteration 135, loss = 0.01881773\n",
      "Iteration 136, loss = 0.01887707\n",
      "Iteration 137, loss = 0.01820597\n",
      "Iteration 138, loss = 0.01833549\n",
      "Iteration 139, loss = 0.01850447\n",
      "Iteration 140, loss = 0.01798277\n",
      "Iteration 141, loss = 0.01886205\n",
      "Iteration 142, loss = 0.01847318\n",
      "Iteration 143, loss = 0.01771302\n",
      "Iteration 144, loss = 0.01758386\n",
      "Iteration 145, loss = 0.01709820\n",
      "Iteration 146, loss = 0.01731016\n",
      "Iteration 147, loss = 0.01615645\n",
      "Iteration 148, loss = 0.01641771\n",
      "Iteration 149, loss = 0.01621226\n",
      "Iteration 150, loss = 0.01620839\n",
      "Iteration 151, loss = 0.01649274\n",
      "Iteration 152, loss = 0.01534652\n",
      "Iteration 153, loss = 0.01604751\n",
      "Iteration 154, loss = 0.01495703\n",
      "Iteration 155, loss = 0.01582382\n",
      "Iteration 156, loss = 0.01452628\n",
      "Iteration 157, loss = 0.01484215\n",
      "Iteration 158, loss = 0.01480999\n",
      "Iteration 159, loss = 0.01463682\n",
      "Iteration 160, loss = 0.01426886\n",
      "Iteration 161, loss = 0.01445200\n",
      "Iteration 162, loss = 0.01508819\n",
      "Iteration 163, loss = 0.01462783\n",
      "Iteration 164, loss = 0.01334073\n",
      "Iteration 165, loss = 0.01382677\n",
      "Iteration 166, loss = 0.01365291\n",
      "Iteration 167, loss = 0.01318636\n",
      "Iteration 168, loss = 0.01327435\n",
      "Iteration 169, loss = 0.01345593\n",
      "Iteration 170, loss = 0.01258199\n",
      "Iteration 171, loss = 0.01294839\n",
      "Iteration 172, loss = 0.01296044\n",
      "Iteration 173, loss = 0.01359093\n",
      "Iteration 174, loss = 0.01283638\n",
      "Iteration 175, loss = 0.01248600\n",
      "Iteration 176, loss = 0.01188282\n",
      "Iteration 177, loss = 0.01179603\n",
      "Iteration 178, loss = 0.01176304\n",
      "Iteration 179, loss = 0.01155747\n",
      "Iteration 180, loss = 0.01144226\n",
      "Iteration 181, loss = 0.01120293\n",
      "Iteration 182, loss = 0.01140765\n",
      "Iteration 183, loss = 0.01127422\n",
      "Iteration 184, loss = 0.01130811\n",
      "Iteration 185, loss = 0.01077956\n",
      "Iteration 186, loss = 0.01141747\n",
      "Iteration 187, loss = 0.01118819\n",
      "Iteration 188, loss = 0.01070379\n",
      "Iteration 189, loss = 0.01045083\n",
      "Iteration 190, loss = 0.01021865\n",
      "Iteration 191, loss = 0.01027348\n",
      "Iteration 192, loss = 0.01035129\n",
      "Iteration 193, loss = 0.01016971\n",
      "Iteration 194, loss = 0.01037286\n",
      "Iteration 195, loss = 0.01047487\n",
      "Iteration 196, loss = 0.00984450\n",
      "Iteration 197, loss = 0.01063503\n",
      "Iteration 198, loss = 0.00984804\n",
      "Iteration 199, loss = 0.00990458\n",
      "Iteration 200, loss = 0.01002500\n",
      "Iteration 201, loss = 0.00991012\n",
      "Iteration 202, loss = 0.00913728\n",
      "Iteration 203, loss = 0.00901717\n",
      "Iteration 204, loss = 0.00903132\n",
      "Iteration 205, loss = 0.00893327\n",
      "Iteration 206, loss = 0.00863681\n",
      "Iteration 207, loss = 0.00900027\n",
      "Iteration 208, loss = 0.00915187\n",
      "Iteration 209, loss = 0.00842502\n",
      "Iteration 210, loss = 0.00841986\n",
      "Iteration 211, loss = 0.00830163\n",
      "Iteration 212, loss = 0.00887542\n",
      "Iteration 213, loss = 0.00874060\n",
      "Iteration 214, loss = 0.00813970\n",
      "Iteration 215, loss = 0.00810920\n",
      "Iteration 216, loss = 0.00815949\n",
      "Iteration 217, loss = 0.00782447\n",
      "Iteration 218, loss = 0.00777408\n",
      "Iteration 219, loss = 0.00763633\n",
      "Iteration 220, loss = 0.00812336\n",
      "Iteration 221, loss = 0.00857318\n",
      "Iteration 222, loss = 0.00804210\n",
      "Iteration 223, loss = 0.00765076\n",
      "Iteration 224, loss = 0.00743994\n",
      "Iteration 225, loss = 0.00718946\n",
      "Iteration 226, loss = 0.00748725\n",
      "Iteration 227, loss = 0.00741926\n",
      "Iteration 228, loss = 0.00714227\n",
      "Iteration 229, loss = 0.00704767\n",
      "Iteration 230, loss = 0.00690488\n",
      "Iteration 231, loss = 0.00712461\n",
      "Iteration 232, loss = 0.00671040\n",
      "Iteration 233, loss = 0.00666125\n",
      "Iteration 234, loss = 0.00729826\n",
      "Iteration 235, loss = 0.00773560\n",
      "Iteration 236, loss = 0.00711157\n",
      "Iteration 237, loss = 0.00653419\n",
      "Iteration 238, loss = 0.00655557\n",
      "Iteration 239, loss = 0.00698395\n",
      "Iteration 240, loss = 0.00690120\n",
      "Iteration 241, loss = 0.00621335\n",
      "Iteration 242, loss = 0.00643772\n",
      "Iteration 243, loss = 0.00616781\n",
      "Iteration 244, loss = 0.00635584\n",
      "Iteration 245, loss = 0.00611584\n",
      "Iteration 246, loss = 0.00615623\n",
      "Iteration 247, loss = 0.00627071\n",
      "Iteration 248, loss = 0.00588616\n",
      "Iteration 249, loss = 0.00622235\n",
      "Iteration 250, loss = 0.00559677\n",
      "Iteration 251, loss = 0.00571963\n",
      "Iteration 252, loss = 0.00570482\n",
      "Iteration 253, loss = 0.00693028\n",
      "Iteration 254, loss = 0.00643819\n",
      "Iteration 255, loss = 0.00600149\n",
      "Iteration 256, loss = 0.00616592\n",
      "Iteration 257, loss = 0.00572208\n",
      "Iteration 258, loss = 0.00576188\n",
      "Iteration 259, loss = 0.00585164\n",
      "Iteration 260, loss = 0.00527484\n",
      "Iteration 261, loss = 0.00519539\n",
      "Iteration 262, loss = 0.00505756\n",
      "Iteration 263, loss = 0.00516587\n",
      "Iteration 264, loss = 0.00501118\n",
      "Iteration 265, loss = 0.00500077\n",
      "Iteration 266, loss = 0.00496532\n",
      "Iteration 267, loss = 0.00498321\n",
      "Iteration 268, loss = 0.00483395\n",
      "Iteration 269, loss = 0.00480890\n",
      "Iteration 270, loss = 0.00483011\n",
      "Iteration 271, loss = 0.00480259\n",
      "Iteration 272, loss = 0.00467563\n",
      "Iteration 273, loss = 0.00471488\n",
      "Iteration 274, loss = 0.00455825\n",
      "Iteration 275, loss = 0.00462979\n",
      "Iteration 276, loss = 0.00452138\n",
      "Iteration 277, loss = 0.00443627\n",
      "Iteration 278, loss = 0.00437211\n",
      "Iteration 279, loss = 0.00447509\n",
      "Iteration 280, loss = 0.00441910\n",
      "Iteration 281, loss = 0.00444394\n",
      "Iteration 282, loss = 0.00435162\n",
      "Iteration 283, loss = 0.00432066\n",
      "Iteration 284, loss = 0.00446854\n",
      "Iteration 285, loss = 0.00425958\n",
      "Iteration 286, loss = 0.00418968\n",
      "Iteration 287, loss = 0.00433063\n",
      "Iteration 288, loss = 0.00416936\n",
      "Iteration 289, loss = 0.00416707\n",
      "Iteration 290, loss = 0.00399727\n",
      "Iteration 291, loss = 0.00390055\n",
      "Iteration 292, loss = 0.00383187\n",
      "Iteration 293, loss = 0.00384321\n",
      "Iteration 294, loss = 0.00379094\n",
      "Iteration 295, loss = 0.00379661\n",
      "Iteration 296, loss = 0.00370688\n",
      "Iteration 297, loss = 0.00368623\n",
      "Iteration 298, loss = 0.00367060\n",
      "Iteration 299, loss = 0.00355478\n",
      "Iteration 300, loss = 0.00370287\n",
      "Iteration 1, loss = 2.90571780\n",
      "Iteration 2, loss = 2.68264630\n",
      "Iteration 3, loss = 2.45877234\n",
      "Iteration 4, loss = 2.21533218\n",
      "Iteration 5, loss = 1.96194370\n",
      "Iteration 6, loss = 1.69745742\n",
      "Iteration 7, loss = 1.42726968\n",
      "Iteration 8, loss = 1.17633104\n",
      "Iteration 9, loss = 0.95403584\n",
      "Iteration 10, loss = 0.77450933\n",
      "Iteration 11, loss = 0.63508798\n",
      "Iteration 12, loss = 0.53127072\n",
      "Iteration 13, loss = 0.45230238\n",
      "Iteration 14, loss = 0.39275914\n",
      "Iteration 15, loss = 0.34675475\n",
      "Iteration 16, loss = 0.30997070\n",
      "Iteration 17, loss = 0.27918056\n",
      "Iteration 18, loss = 0.25464826\n",
      "Iteration 19, loss = 0.23407011\n",
      "Iteration 20, loss = 0.21641505\n",
      "Iteration 21, loss = 0.20173530\n",
      "Iteration 22, loss = 0.18903631\n",
      "Iteration 23, loss = 0.17729880\n",
      "Iteration 24, loss = 0.16751049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25, loss = 0.15896388\n",
      "Iteration 26, loss = 0.15110012\n",
      "Iteration 27, loss = 0.14317607\n",
      "Iteration 28, loss = 0.13683567\n",
      "Iteration 29, loss = 0.13149328\n",
      "Iteration 30, loss = 0.12548660\n",
      "Iteration 31, loss = 0.12127192\n",
      "Iteration 32, loss = 0.11691873\n",
      "Iteration 33, loss = 0.11317743\n",
      "Iteration 34, loss = 0.10847276\n",
      "Iteration 35, loss = 0.10547825\n",
      "Iteration 36, loss = 0.10241675\n",
      "Iteration 37, loss = 0.09876909\n",
      "Iteration 38, loss = 0.09629891\n",
      "Iteration 39, loss = 0.09371361\n",
      "Iteration 40, loss = 0.09132436\n",
      "Iteration 41, loss = 0.08822473\n",
      "Iteration 42, loss = 0.08674883\n",
      "Iteration 43, loss = 0.08497737\n",
      "Iteration 44, loss = 0.08207854\n",
      "Iteration 45, loss = 0.08040806\n",
      "Iteration 46, loss = 0.07907463\n",
      "Iteration 47, loss = 0.07597291\n",
      "Iteration 48, loss = 0.07564372\n",
      "Iteration 49, loss = 0.07359921\n",
      "Iteration 50, loss = 0.07243877\n",
      "Iteration 51, loss = 0.07006198\n",
      "Iteration 52, loss = 0.06879876\n",
      "Iteration 53, loss = 0.06730678\n",
      "Iteration 54, loss = 0.06571541\n",
      "Iteration 55, loss = 0.06485263\n",
      "Iteration 56, loss = 0.06332970\n",
      "Iteration 57, loss = 0.06276401\n",
      "Iteration 58, loss = 0.06089692\n",
      "Iteration 59, loss = 0.06024016\n",
      "Iteration 60, loss = 0.05878222\n",
      "Iteration 61, loss = 0.05756749\n",
      "Iteration 62, loss = 0.05612698\n",
      "Iteration 63, loss = 0.05510785\n",
      "Iteration 64, loss = 0.05445465\n",
      "Iteration 65, loss = 0.05423654\n",
      "Iteration 66, loss = 0.05175573\n",
      "Iteration 67, loss = 0.05153335\n",
      "Iteration 68, loss = 0.05045405\n",
      "Iteration 69, loss = 0.04907724\n",
      "Iteration 70, loss = 0.04864955\n",
      "Iteration 71, loss = 0.04782530\n",
      "Iteration 72, loss = 0.04672999\n",
      "Iteration 73, loss = 0.04579929\n",
      "Iteration 74, loss = 0.04500068\n",
      "Iteration 75, loss = 0.04432268\n",
      "Iteration 76, loss = 0.04330132\n",
      "Iteration 77, loss = 0.04265374\n",
      "Iteration 78, loss = 0.04252095\n",
      "Iteration 79, loss = 0.04181362\n",
      "Iteration 80, loss = 0.04074958\n",
      "Iteration 81, loss = 0.04057698\n",
      "Iteration 82, loss = 0.04000584\n",
      "Iteration 83, loss = 0.03830444\n",
      "Iteration 84, loss = 0.03775853\n",
      "Iteration 85, loss = 0.03738117\n",
      "Iteration 86, loss = 0.03669473\n",
      "Iteration 87, loss = 0.03577168\n",
      "Iteration 88, loss = 0.03609922\n",
      "Iteration 89, loss = 0.03508982\n",
      "Iteration 90, loss = 0.03534505\n",
      "Iteration 91, loss = 0.03408766\n",
      "Iteration 92, loss = 0.03332571\n",
      "Iteration 93, loss = 0.03265506\n",
      "Iteration 94, loss = 0.03234919\n",
      "Iteration 95, loss = 0.03126113\n",
      "Iteration 96, loss = 0.03106343\n",
      "Iteration 97, loss = 0.03030313\n",
      "Iteration 98, loss = 0.03051850\n",
      "Iteration 99, loss = 0.03042388\n",
      "Iteration 100, loss = 0.02976863\n",
      "Iteration 101, loss = 0.02927563\n",
      "Iteration 102, loss = 0.02816086\n",
      "Iteration 103, loss = 0.02829499\n",
      "Iteration 104, loss = 0.02794624\n",
      "Iteration 105, loss = 0.02772536\n",
      "Iteration 106, loss = 0.02728905\n",
      "Iteration 107, loss = 0.02790181\n",
      "Iteration 108, loss = 0.02985588\n",
      "Iteration 109, loss = 0.02664086\n",
      "Iteration 110, loss = 0.02512643\n",
      "Iteration 111, loss = 0.02524372\n",
      "Iteration 112, loss = 0.02484309\n",
      "Iteration 113, loss = 0.02464937\n",
      "Iteration 114, loss = 0.02383987\n",
      "Iteration 115, loss = 0.02349570\n",
      "Iteration 116, loss = 0.02336301\n",
      "Iteration 117, loss = 0.02308172\n",
      "Iteration 118, loss = 0.02261853\n",
      "Iteration 119, loss = 0.02324362\n",
      "Iteration 120, loss = 0.02209583\n",
      "Iteration 121, loss = 0.02245276\n",
      "Iteration 122, loss = 0.02115057\n",
      "Iteration 123, loss = 0.02100839\n",
      "Iteration 124, loss = 0.02110286\n",
      "Iteration 125, loss = 0.02090964\n",
      "Iteration 126, loss = 0.02130213\n",
      "Iteration 127, loss = 0.02078113\n",
      "Iteration 128, loss = 0.02046768\n",
      "Iteration 129, loss = 0.02000481\n",
      "Iteration 130, loss = 0.01932507\n",
      "Iteration 131, loss = 0.01896139\n",
      "Iteration 132, loss = 0.01891743\n",
      "Iteration 133, loss = 0.01880050\n",
      "Iteration 134, loss = 0.01894385\n",
      "Iteration 135, loss = 0.01835828\n",
      "Iteration 136, loss = 0.01811895\n",
      "Iteration 137, loss = 0.01775954\n",
      "Iteration 138, loss = 0.01717391\n",
      "Iteration 139, loss = 0.01733857\n",
      "Iteration 140, loss = 0.01692297\n",
      "Iteration 141, loss = 0.01679464\n",
      "Iteration 142, loss = 0.01659070\n",
      "Iteration 143, loss = 0.01645466\n",
      "Iteration 144, loss = 0.01602878\n",
      "Iteration 145, loss = 0.01583474\n",
      "Iteration 146, loss = 0.01570904\n",
      "Iteration 147, loss = 0.01579915\n",
      "Iteration 148, loss = 0.01518245\n",
      "Iteration 149, loss = 0.01539131\n",
      "Iteration 150, loss = 0.01572921\n",
      "Iteration 151, loss = 0.01534300\n",
      "Iteration 152, loss = 0.01510042\n",
      "Iteration 153, loss = 0.01498809\n",
      "Iteration 154, loss = 0.01569332\n",
      "Iteration 155, loss = 0.01416090\n",
      "Iteration 156, loss = 0.01490275\n",
      "Iteration 157, loss = 0.01605073\n",
      "Iteration 158, loss = 0.01482215\n",
      "Iteration 159, loss = 0.01443926\n",
      "Iteration 160, loss = 0.01327585\n",
      "Iteration 161, loss = 0.01407010\n",
      "Iteration 162, loss = 0.01439755\n",
      "Iteration 163, loss = 0.01436972\n",
      "Iteration 164, loss = 0.01336439\n",
      "Iteration 165, loss = 0.01283973\n",
      "Iteration 166, loss = 0.01269249\n",
      "Iteration 167, loss = 0.01250705\n",
      "Iteration 168, loss = 0.01257796\n",
      "Iteration 169, loss = 0.01246289\n",
      "Iteration 170, loss = 0.01239343\n",
      "Iteration 171, loss = 0.01354015\n",
      "Iteration 172, loss = 0.01318868\n",
      "Iteration 173, loss = 0.01309869\n",
      "Iteration 174, loss = 0.01259321\n",
      "Iteration 175, loss = 0.01103668\n",
      "Iteration 176, loss = 0.01219556\n",
      "Iteration 177, loss = 0.01127327\n",
      "Iteration 178, loss = 0.01146108\n",
      "Iteration 179, loss = 0.01222605\n",
      "Iteration 180, loss = 0.01111641\n",
      "Iteration 181, loss = 0.01094861\n",
      "Iteration 182, loss = 0.01094712\n",
      "Iteration 183, loss = 0.01059837\n",
      "Iteration 184, loss = 0.01068533\n",
      "Iteration 185, loss = 0.01043061\n",
      "Iteration 186, loss = 0.01014912\n",
      "Iteration 187, loss = 0.01036705\n",
      "Iteration 188, loss = 0.01000465\n",
      "Iteration 189, loss = 0.01008670\n",
      "Iteration 190, loss = 0.00987511\n",
      "Iteration 191, loss = 0.00980550\n",
      "Iteration 192, loss = 0.00959788\n",
      "Iteration 193, loss = 0.00971680\n",
      "Iteration 194, loss = 0.00983518\n",
      "Iteration 195, loss = 0.00949439\n",
      "Iteration 196, loss = 0.00924036\n",
      "Iteration 197, loss = 0.00966795\n",
      "Iteration 198, loss = 0.00941169\n",
      "Iteration 199, loss = 0.00914823\n",
      "Iteration 200, loss = 0.00896028\n",
      "Iteration 201, loss = 0.00874683\n",
      "Iteration 202, loss = 0.00872319\n",
      "Iteration 203, loss = 0.00867933\n",
      "Iteration 204, loss = 0.00843017\n",
      "Iteration 205, loss = 0.00853211\n",
      "Iteration 206, loss = 0.00850522\n",
      "Iteration 207, loss = 0.00933392\n",
      "Iteration 208, loss = 0.00921271\n",
      "Iteration 209, loss = 0.00846182\n",
      "Iteration 210, loss = 0.00825962\n",
      "Iteration 211, loss = 0.00830123\n",
      "Iteration 212, loss = 0.00846637\n",
      "Iteration 213, loss = 0.00861455\n",
      "Iteration 214, loss = 0.00779459\n",
      "Iteration 215, loss = 0.00804582\n",
      "Iteration 216, loss = 0.00801470\n",
      "Iteration 217, loss = 0.00759173\n",
      "Iteration 218, loss = 0.00762173\n",
      "Iteration 219, loss = 0.00745268\n",
      "Iteration 220, loss = 0.00733020\n",
      "Iteration 221, loss = 0.00772824\n",
      "Iteration 222, loss = 0.00762679\n",
      "Iteration 223, loss = 0.00729405\n",
      "Iteration 224, loss = 0.00710045\n",
      "Iteration 225, loss = 0.00707626\n",
      "Iteration 226, loss = 0.00697362\n",
      "Iteration 227, loss = 0.00716446\n",
      "Iteration 228, loss = 0.00744209\n",
      "Iteration 229, loss = 0.00805658\n",
      "Iteration 230, loss = 0.00702945\n",
      "Iteration 231, loss = 0.00692626\n",
      "Iteration 232, loss = 0.00655067\n",
      "Iteration 233, loss = 0.00669254\n",
      "Iteration 234, loss = 0.00705017\n",
      "Iteration 235, loss = 0.00703904\n",
      "Iteration 236, loss = 0.00707453\n",
      "Iteration 237, loss = 0.00642807\n",
      "Iteration 238, loss = 0.00626457\n",
      "Iteration 239, loss = 0.00638647\n",
      "Iteration 240, loss = 0.00627243\n",
      "Iteration 241, loss = 0.00625861\n",
      "Iteration 242, loss = 0.00572867\n",
      "Iteration 243, loss = 0.00632943\n",
      "Iteration 244, loss = 0.00626889\n",
      "Iteration 245, loss = 0.00590828\n",
      "Iteration 246, loss = 0.00593107\n",
      "Iteration 247, loss = 0.00583255\n",
      "Iteration 248, loss = 0.00589199\n",
      "Iteration 249, loss = 0.00622065\n",
      "Iteration 250, loss = 0.00578939\n",
      "Iteration 251, loss = 0.00571260\n",
      "Iteration 252, loss = 0.00551704\n",
      "Iteration 253, loss = 0.00557256\n",
      "Iteration 254, loss = 0.00554816\n",
      "Iteration 255, loss = 0.00542848\n",
      "Iteration 256, loss = 0.00534121\n",
      "Iteration 257, loss = 0.00524914\n",
      "Iteration 258, loss = 0.00509483\n",
      "Iteration 259, loss = 0.00548211\n",
      "Iteration 260, loss = 0.00517757\n",
      "Iteration 261, loss = 0.00504641\n",
      "Iteration 262, loss = 0.00492764\n",
      "Iteration 263, loss = 0.00517926\n",
      "Iteration 264, loss = 0.00518228\n",
      "Iteration 265, loss = 0.00507497\n",
      "Iteration 266, loss = 0.00487155\n",
      "Iteration 267, loss = 0.00477613\n",
      "Iteration 268, loss = 0.00469384\n",
      "Iteration 269, loss = 0.00473931\n",
      "Iteration 270, loss = 0.00470703\n",
      "Iteration 271, loss = 0.00458178\n",
      "Iteration 272, loss = 0.00454313\n",
      "Iteration 273, loss = 0.00460858\n",
      "Iteration 274, loss = 0.00464283\n",
      "Iteration 275, loss = 0.00443890\n",
      "Iteration 276, loss = 0.00443150\n",
      "Iteration 277, loss = 0.00457296\n",
      "Iteration 278, loss = 0.00437647\n",
      "Iteration 279, loss = 0.00427296\n",
      "Iteration 280, loss = 0.00430727\n",
      "Iteration 281, loss = 0.00412439\n",
      "Iteration 282, loss = 0.00452258\n",
      "Iteration 283, loss = 0.00441723\n",
      "Iteration 284, loss = 0.00404763\n",
      "Iteration 285, loss = 0.00407076\n",
      "Iteration 286, loss = 0.00432239\n",
      "Iteration 287, loss = 0.00409914\n",
      "Iteration 288, loss = 0.00429380\n",
      "Iteration 289, loss = 0.00402551\n",
      "Iteration 290, loss = 0.00394957\n",
      "Iteration 291, loss = 0.00389740\n",
      "Iteration 292, loss = 0.00388622\n",
      "Iteration 293, loss = 0.00392983\n",
      "Iteration 294, loss = 0.00371389\n",
      "Iteration 295, loss = 0.00368879\n",
      "Iteration 296, loss = 0.00373241\n",
      "Iteration 297, loss = 0.00387158\n",
      "Iteration 298, loss = 0.00393552\n",
      "Iteration 299, loss = 0.00381996\n",
      "Iteration 300, loss = 0.00365758\n",
      "Iteration 1, loss = 2.90480217\n",
      "Iteration 2, loss = 2.68547831\n",
      "Iteration 3, loss = 2.46491514\n",
      "Iteration 4, loss = 2.22480061\n",
      "Iteration 5, loss = 1.97409415\n",
      "Iteration 6, loss = 1.70908666\n",
      "Iteration 7, loss = 1.43912757\n",
      "Iteration 8, loss = 1.18659632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.96397047\n",
      "Iteration 10, loss = 0.78345250\n",
      "Iteration 11, loss = 0.64226136\n",
      "Iteration 12, loss = 0.53678633\n",
      "Iteration 13, loss = 0.45675970\n",
      "Iteration 14, loss = 0.39542953\n",
      "Iteration 15, loss = 0.34772636\n",
      "Iteration 16, loss = 0.31202942\n",
      "Iteration 17, loss = 0.28133595\n",
      "Iteration 18, loss = 0.25786619\n",
      "Iteration 19, loss = 0.23847314\n",
      "Iteration 20, loss = 0.22158405\n",
      "Iteration 21, loss = 0.20552969\n",
      "Iteration 22, loss = 0.19362738\n",
      "Iteration 23, loss = 0.18168246\n",
      "Iteration 24, loss = 0.17166956\n",
      "Iteration 25, loss = 0.16231650\n",
      "Iteration 26, loss = 0.15470725\n",
      "Iteration 27, loss = 0.14749219\n",
      "Iteration 28, loss = 0.14039364\n",
      "Iteration 29, loss = 0.13473703\n",
      "Iteration 30, loss = 0.12897673\n",
      "Iteration 31, loss = 0.12327212\n",
      "Iteration 32, loss = 0.11895118\n",
      "Iteration 33, loss = 0.11587368\n",
      "Iteration 34, loss = 0.10948889\n",
      "Iteration 35, loss = 0.10723060\n",
      "Iteration 36, loss = 0.10325689\n",
      "Iteration 37, loss = 0.09946711\n",
      "Iteration 38, loss = 0.09694607\n",
      "Iteration 39, loss = 0.09379574\n",
      "Iteration 40, loss = 0.09218104\n",
      "Iteration 41, loss = 0.08913337\n",
      "Iteration 42, loss = 0.08716805\n",
      "Iteration 43, loss = 0.08486352\n",
      "Iteration 44, loss = 0.08191789\n",
      "Iteration 45, loss = 0.08003782\n",
      "Iteration 46, loss = 0.07939637\n",
      "Iteration 47, loss = 0.07603991\n",
      "Iteration 48, loss = 0.07528892\n",
      "Iteration 49, loss = 0.07279744\n",
      "Iteration 50, loss = 0.07242755\n",
      "Iteration 51, loss = 0.07008054\n",
      "Iteration 52, loss = 0.06794752\n",
      "Iteration 53, loss = 0.06676912\n",
      "Iteration 54, loss = 0.06543651\n",
      "Iteration 55, loss = 0.06369515\n",
      "Iteration 56, loss = 0.06249002\n",
      "Iteration 57, loss = 0.06105320\n",
      "Iteration 58, loss = 0.05940186\n",
      "Iteration 59, loss = 0.05841620\n",
      "Iteration 60, loss = 0.05737959\n",
      "Iteration 61, loss = 0.05575272\n",
      "Iteration 62, loss = 0.05473087\n",
      "Iteration 63, loss = 0.05335316\n",
      "Iteration 64, loss = 0.05238377\n",
      "Iteration 65, loss = 0.05116273\n",
      "Iteration 66, loss = 0.05067614\n",
      "Iteration 67, loss = 0.04970097\n",
      "Iteration 68, loss = 0.04895553\n",
      "Iteration 69, loss = 0.04691778\n",
      "Iteration 70, loss = 0.04684028\n",
      "Iteration 71, loss = 0.04699624\n",
      "Iteration 72, loss = 0.04534576\n",
      "Iteration 73, loss = 0.04406116\n",
      "Iteration 74, loss = 0.04331246\n",
      "Iteration 75, loss = 0.04239110\n",
      "Iteration 76, loss = 0.04172170\n",
      "Iteration 77, loss = 0.04104681\n",
      "Iteration 78, loss = 0.04083529\n",
      "Iteration 79, loss = 0.04042011\n",
      "Iteration 80, loss = 0.03878383\n",
      "Iteration 81, loss = 0.03861838\n",
      "Iteration 82, loss = 0.03741311\n",
      "Iteration 83, loss = 0.03728399\n",
      "Iteration 84, loss = 0.03800265\n",
      "Iteration 85, loss = 0.03671505\n",
      "Iteration 86, loss = 0.03583681\n",
      "Iteration 87, loss = 0.03423674\n",
      "Iteration 88, loss = 0.03406406\n",
      "Iteration 89, loss = 0.03286128\n",
      "Iteration 90, loss = 0.03289024\n",
      "Iteration 91, loss = 0.03196756\n",
      "Iteration 92, loss = 0.03122944\n",
      "Iteration 93, loss = 0.03097310\n",
      "Iteration 94, loss = 0.03008428\n",
      "Iteration 95, loss = 0.03027507\n",
      "Iteration 96, loss = 0.03011567\n",
      "Iteration 97, loss = 0.02869577\n",
      "Iteration 98, loss = 0.02862330\n",
      "Iteration 99, loss = 0.02868382\n",
      "Iteration 100, loss = 0.02751594\n",
      "Iteration 101, loss = 0.02708912\n",
      "Iteration 102, loss = 0.02654501\n",
      "Iteration 103, loss = 0.02646158\n",
      "Iteration 104, loss = 0.02594851\n",
      "Iteration 105, loss = 0.02583397\n",
      "Iteration 106, loss = 0.02517004\n",
      "Iteration 107, loss = 0.02544232\n",
      "Iteration 108, loss = 0.02549120\n",
      "Iteration 109, loss = 0.02387307\n",
      "Iteration 110, loss = 0.02375211\n",
      "Iteration 111, loss = 0.02339632\n",
      "Iteration 112, loss = 0.02293248\n",
      "Iteration 113, loss = 0.02322622\n",
      "Iteration 114, loss = 0.02323322\n",
      "Iteration 115, loss = 0.02181644\n",
      "Iteration 116, loss = 0.02206497\n",
      "Iteration 117, loss = 0.02134828\n",
      "Iteration 118, loss = 0.02129459\n",
      "Iteration 119, loss = 0.02088718\n",
      "Iteration 120, loss = 0.02055631\n",
      "Iteration 121, loss = 0.02034025\n",
      "Iteration 122, loss = 0.02005968\n",
      "Iteration 123, loss = 0.02005195\n",
      "Iteration 124, loss = 0.01982813\n",
      "Iteration 125, loss = 0.01979564\n",
      "Iteration 126, loss = 0.02034802\n",
      "Iteration 127, loss = 0.01954912\n",
      "Iteration 128, loss = 0.01899379\n",
      "Iteration 129, loss = 0.01897940\n",
      "Iteration 130, loss = 0.01834187\n",
      "Iteration 131, loss = 0.01815643\n",
      "Iteration 132, loss = 0.01758599\n",
      "Iteration 133, loss = 0.01762462\n",
      "Iteration 134, loss = 0.01735258\n",
      "Iteration 135, loss = 0.01781312\n",
      "Iteration 136, loss = 0.01744123\n",
      "Iteration 137, loss = 0.01693215\n",
      "Iteration 138, loss = 0.01652618\n",
      "Iteration 139, loss = 0.01644554\n",
      "Iteration 140, loss = 0.01626150\n",
      "Iteration 141, loss = 0.01571287\n",
      "Iteration 142, loss = 0.01588195\n",
      "Iteration 143, loss = 0.01583985\n",
      "Iteration 144, loss = 0.01509882\n",
      "Iteration 145, loss = 0.01498770\n",
      "Iteration 146, loss = 0.01501984\n",
      "Iteration 147, loss = 0.01478819\n",
      "Iteration 148, loss = 0.01529437\n",
      "Iteration 149, loss = 0.01666857\n",
      "Iteration 150, loss = 0.01517247\n",
      "Iteration 151, loss = 0.01450366\n",
      "Iteration 152, loss = 0.01416263\n",
      "Iteration 153, loss = 0.01391756\n",
      "Iteration 154, loss = 0.01356882\n",
      "Iteration 155, loss = 0.01375956\n",
      "Iteration 156, loss = 0.01409995\n",
      "Iteration 157, loss = 0.01347567\n",
      "Iteration 158, loss = 0.01293015\n",
      "Iteration 159, loss = 0.01365307\n",
      "Iteration 160, loss = 0.01246747\n",
      "Iteration 161, loss = 0.01322282\n",
      "Iteration 162, loss = 0.01345595\n",
      "Iteration 163, loss = 0.01334074\n",
      "Iteration 164, loss = 0.01219410\n",
      "Iteration 165, loss = 0.01205568\n",
      "Iteration 166, loss = 0.01220683\n",
      "Iteration 167, loss = 0.01197597\n",
      "Iteration 168, loss = 0.01205052\n",
      "Iteration 169, loss = 0.01215405\n",
      "Iteration 170, loss = 0.01164591\n",
      "Iteration 171, loss = 0.01150485\n",
      "Iteration 172, loss = 0.01172636\n",
      "Iteration 173, loss = 0.01147166\n",
      "Iteration 174, loss = 0.01108690\n",
      "Iteration 175, loss = 0.01083052\n",
      "Iteration 176, loss = 0.01108293\n",
      "Iteration 177, loss = 0.01111405\n",
      "Iteration 178, loss = 0.01060897\n",
      "Iteration 179, loss = 0.01116366\n",
      "Iteration 180, loss = 0.01073158\n",
      "Iteration 181, loss = 0.01100907\n",
      "Iteration 182, loss = 0.01120030\n",
      "Iteration 183, loss = 0.01058332\n",
      "Iteration 184, loss = 0.01020882\n",
      "Iteration 185, loss = 0.00976193\n",
      "Iteration 186, loss = 0.01035062\n",
      "Iteration 187, loss = 0.01052329\n",
      "Iteration 188, loss = 0.01005478\n",
      "Iteration 189, loss = 0.00979322\n",
      "Iteration 190, loss = 0.00964994\n",
      "Iteration 191, loss = 0.00951281\n",
      "Iteration 192, loss = 0.00933304\n",
      "Iteration 193, loss = 0.00965995\n",
      "Iteration 194, loss = 0.00922165\n",
      "Iteration 195, loss = 0.00908329\n",
      "Iteration 196, loss = 0.00904098\n",
      "Iteration 197, loss = 0.00883215\n",
      "Iteration 198, loss = 0.00889109\n",
      "Iteration 199, loss = 0.00914856\n",
      "Iteration 200, loss = 0.00870672\n",
      "Iteration 201, loss = 0.00852800\n",
      "Iteration 202, loss = 0.00867272\n",
      "Iteration 203, loss = 0.00867240\n",
      "Iteration 204, loss = 0.00844285\n",
      "Iteration 205, loss = 0.00835396\n",
      "Iteration 206, loss = 0.00814388\n",
      "Iteration 207, loss = 0.00921019\n",
      "Iteration 208, loss = 0.00887208\n",
      "Iteration 209, loss = 0.00825983\n",
      "Iteration 210, loss = 0.00817997\n",
      "Iteration 211, loss = 0.00819355\n",
      "Iteration 212, loss = 0.00777288\n",
      "Iteration 213, loss = 0.00811513\n",
      "Iteration 214, loss = 0.00792513\n",
      "Iteration 215, loss = 0.00757493\n",
      "Iteration 216, loss = 0.00757874\n",
      "Iteration 217, loss = 0.00734035\n",
      "Iteration 218, loss = 0.00741315\n",
      "Iteration 219, loss = 0.00731503\n",
      "Iteration 220, loss = 0.00740542\n",
      "Iteration 221, loss = 0.00818847\n",
      "Iteration 222, loss = 0.00749436\n",
      "Iteration 223, loss = 0.00721586\n",
      "Iteration 224, loss = 0.00742977\n",
      "Iteration 225, loss = 0.00677233\n",
      "Iteration 226, loss = 0.00715269\n",
      "Iteration 227, loss = 0.00685978\n",
      "Iteration 228, loss = 0.00686963\n",
      "Iteration 229, loss = 0.00706044\n",
      "Iteration 230, loss = 0.00683924\n",
      "Iteration 231, loss = 0.00684226\n",
      "Iteration 232, loss = 0.00659812\n",
      "Iteration 233, loss = 0.00669951\n",
      "Iteration 234, loss = 0.00643411\n",
      "Iteration 235, loss = 0.00723123\n",
      "Iteration 236, loss = 0.00654982\n",
      "Iteration 237, loss = 0.00625985\n",
      "Iteration 238, loss = 0.00629361\n",
      "Iteration 239, loss = 0.00631117\n",
      "Iteration 240, loss = 0.00642517\n",
      "Iteration 241, loss = 0.00716209\n",
      "Iteration 242, loss = 0.00637114\n",
      "Iteration 243, loss = 0.00603983\n",
      "Iteration 244, loss = 0.00639800\n",
      "Iteration 245, loss = 0.00633576\n",
      "Iteration 246, loss = 0.00618681\n",
      "Iteration 247, loss = 0.00617078\n",
      "Iteration 248, loss = 0.00588934\n",
      "Iteration 249, loss = 0.00556414\n",
      "Iteration 250, loss = 0.00568043\n",
      "Iteration 251, loss = 0.00593944\n",
      "Iteration 252, loss = 0.00543708\n",
      "Iteration 253, loss = 0.00568988\n",
      "Iteration 254, loss = 0.00537664\n",
      "Iteration 255, loss = 0.00553922\n",
      "Iteration 256, loss = 0.00545872\n",
      "Iteration 257, loss = 0.00544069\n",
      "Iteration 258, loss = 0.00559117\n",
      "Iteration 259, loss = 0.00571612\n",
      "Iteration 260, loss = 0.00522982\n",
      "Iteration 261, loss = 0.00509260\n",
      "Iteration 262, loss = 0.00514265\n",
      "Iteration 263, loss = 0.00512169\n",
      "Iteration 264, loss = 0.00513848\n",
      "Iteration 265, loss = 0.00508216\n",
      "Iteration 266, loss = 0.00485338\n",
      "Iteration 267, loss = 0.00490476\n",
      "Iteration 268, loss = 0.00487579\n",
      "Iteration 269, loss = 0.00505267\n",
      "Iteration 270, loss = 0.00485258\n",
      "Iteration 271, loss = 0.00484142\n",
      "Iteration 272, loss = 0.00468445\n",
      "Iteration 273, loss = 0.00466314\n",
      "Iteration 274, loss = 0.00471661\n",
      "Iteration 275, loss = 0.00480398\n",
      "Iteration 276, loss = 0.00464423\n",
      "Iteration 277, loss = 0.00495433\n",
      "Iteration 278, loss = 0.00455662\n",
      "Iteration 279, loss = 0.00449099\n",
      "Iteration 280, loss = 0.00455248\n",
      "Iteration 281, loss = 0.00446703\n",
      "Iteration 282, loss = 0.00441704\n",
      "Iteration 283, loss = 0.00452796\n",
      "Iteration 284, loss = 0.00432330\n",
      "Iteration 285, loss = 0.00419647\n",
      "Iteration 286, loss = 0.00447593\n",
      "Iteration 287, loss = 0.00483408\n",
      "Iteration 288, loss = 0.00433250\n",
      "Iteration 289, loss = 0.00428169\n",
      "Iteration 290, loss = 0.00420090\n",
      "Iteration 291, loss = 0.00429020\n",
      "Iteration 292, loss = 0.00408249\n",
      "Iteration 293, loss = 0.00398915\n",
      "Iteration 294, loss = 0.00413770\n",
      "Iteration 295, loss = 0.00390274\n",
      "Iteration 296, loss = 0.00387038\n",
      "Iteration 297, loss = 0.00394635\n",
      "Iteration 298, loss = 0.00379718\n",
      "Iteration 299, loss = 0.00392737\n",
      "Iteration 300, loss = 0.00397372\n",
      "Accuracy: 0.9896373056994818\n",
      "Cross-validation scores: [0.98445596 0.98963731 0.98961039 0.99220779 0.98961039]\n",
      "Mean CV accuracy: 0.9891043671354552\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        17\n",
      "           2       0.95      0.95      0.95        22\n",
      "           3       1.00      1.00      1.00        23\n",
      "           4       1.00      1.00      1.00        19\n",
      "           5       1.00      1.00      1.00        30\n",
      "           6       1.00      1.00      1.00        18\n",
      "           8       0.94      1.00      0.97        16\n",
      "           9       1.00      1.00      1.00        24\n",
      "          10       0.88      1.00      0.93        14\n",
      "          11       1.00      1.00      1.00        14\n",
      "          12       1.00      1.00      1.00        16\n",
      "          13       1.00      0.94      0.97        16\n",
      "          14       1.00      1.00      1.00        20\n",
      "          15       1.00      1.00      1.00        20\n",
      "          16       1.00      1.00      1.00        19\n",
      "          17       1.00      1.00      1.00        20\n",
      "          18       1.00      0.95      0.98        22\n",
      "          19       1.00      1.00      1.00        18\n",
      "          20       1.00      0.94      0.97        17\n",
      "          21       1.00      1.00      1.00        21\n",
      "\n",
      "    accuracy                           0.99       386\n",
      "   macro avg       0.99      0.99      0.99       386\n",
      "weighted avg       0.99      0.99      0.99       386\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cv_scores = cross_val_score(mlp, np.vstack((X_train_scaled, X_test_scaled)), np.hstack((y_train, y_test)), cv=5)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", cv_scores.mean())\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
